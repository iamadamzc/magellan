# ML Position Sizing - REVISED Quick Start

**Location:** `research/ml_position_sizing/`  
**Status:** Framework Ready (Corrected Labeling)  
**Created:** 2026-01-19

---

## âš ï¸ **CRITICAL FIX: No Look-Ahead Bias Labeling**

**Original Approach (WRONG):**
- Label based on outcomes ("This trade won 2R, so call it ADD_ALLOWED")
- âŒ Conceptual look-ahead bias
- âŒ Teaches model to predict outcomes, not recognize regimes

**Corrected Approach (RIGHT):**
- Label based on ENTRY-TIME structure only
- âœ… No look-ahead bias
- âœ… Teaches model to recognize environments

**See:** `LABELING_PROTOCOL.md` for full explanation

---

## ğŸš€ **Quick Start (3 Steps)**

### **Step 1: Extract Historical Trades**
```powershell
cd a:\1\Magellan
python research\ml_position_sizing\scripts\extract_bear_trap_trades.py
```
**Time:** ~10 minutes  
**Output:** `data/bear_trap_trades_2020_2024.csv`  
**What it does:** Runs Bear Trap on 2020-2024, extracts all trades with entry-time features

---

### **Step 2: Label Trades (AUTOMATED - No Manual Work!)**
```powershell
python research\ml_position_sizing\scripts\label_regime_structural.py
```
**Time:** ~1 minute  
**Output:** `data/labeled_regimes.csv`  
**What it does:** 
- Calculates 5-component structural score (0-15 points)
- Maps to regime label (ADD_ALLOWED, ADD_NEUTRAL, NO_ADD)
- Validates correlation with outcomes
- Provides sanity checks

**5 Structural Components (All Entry-Time):**
1. **Trend Strength** - Higher highs count before entry
2. **Volatility Regime** - ATR percentile
3. **Volume Confirmation** - Volume vs 20-bar average
4. **Drop Severity** - How big was the drop (Bear Trap specific)
5. **Recent Performance** - Last 5 trades win rate (meta-level)

**Scoring:**
- 11-15 points â†’ ADD_ALLOWED (strong setup)
- 7-10 points â†’ ADD_NEUTRAL (mixed)
- 0-6 points â†’ NO_ADD (weak setup)

---

### **Step 3: Validate Labels**
```powershell
# Review the output from Step 2
cat research\ml_position_sizing\data\validation_report.txt
```

**Check for:**
- âœ… ADD_ALLOWED avg R-multiple > NO_ADD avg R-multiple
- âœ… Quality score > 0
- âœ… Label distribution makes sense (~30% each)

**If validation FAILS:**
- âŒ Don't relabel based on outcomes!
- âœ… Revise structural features in `label_regime_structural.py`
- âœ… Re-run and validate again

---

## ğŸ“Š **Expected Validation Results**

**Good labeling looks like:**
```
ADD_ALLOWED:
  Avg R-multiple: +1.2
  Win rate: 65%

ADD_NEUTRAL:
  Avg R-multiple: +0.5
  Win rate: 55%

NO_ADD:
  Avg R-multiple: -0.2
  Win rate: 45%

Quality Score: +1.4 R âœ…
```

**Bad labeling looks like:**
```
ADD_ALLOWED:
  Avg R-multiple: -0.5
  Win rate: 40%

NO_ADD:
  Avg R-multiple: +1.0
  Win rate: 60%

Quality Score: -1.5 R âš ï¸ (features are backwards!)
```

---

## ğŸ§  **After Validation Passes:**

### **Step 4: Train ML Model** (Next session)
```powershell
python research\ml_position_sizing\scripts\train_regime_model.py
```
**What it does:**
- Trains decision tree on structural features
- Uses labeled data from Step 2
- Validates on holdout set

### **Step 5: Backtest** (Next session)
```powershell
python research\ml_position_sizing\scripts\backtest_ml_bear_trap.py
```
**What it does:**
- Runs Bear Trap with ML regime selection 
- Compares vs baseline
- Generates performance report

---

## ğŸ¯ **Key Principle**

> **Labels describe the ENVIRONMENT at entry, not the RESULT after entry.**

**Analogy:**
- âœ… GOOD: "Storm clouds present" (observable now)
- âŒ BAD: "Will rain in 2 hours" (prediction)

**In Trading:**
- âœ… GOOD: "Strong trend + high volume + quick reclaim" (observable at entry)
- âŒ BAD: "This trade will win 2R" (outcome-based)

---

## âš ï¸ **Common Pitfalls**

### **Pitfall 1: Relabeling Based on Outcomes**
```python
# âŒ WRONG
if trade_won_big:
    regime = 'ADD_ALLOWED'  # This is look-ahead bias!

# âœ… RIGHT
if structural_score >= 11:
    regime = 'ADD_ALLOWED'  # Based on entry conditions
```

### **Pitfall 2: Using Future Information**
```python
# âŒ WRONG - These are AFTER entry
if max_favorable_excursion > 5%:
    label = 'ADD_ALLOWED'

# âœ… RIGHT - These are AT/BEFORE entry
if volume_at_entry > avg_volume * 1.5:
    score += 2
```

### **Pitfall 3: Forcing Labels to Match Outcomes**
```python
# If validation fails:
# âŒ WRONG: Change labels to match outcomes
# âœ… RIGHT: Change structural features to better predict regime
```

---

## ğŸ“‚ **File Structure**

```
ml_position_sizing/
â”œâ”€â”€ README.md                          # Full framework
â”œâ”€â”€ QUICK_START.md                     # This file
â”œâ”€â”€ LABELING_PROTOCOL.md               # Critical fix explanation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bear_trap_trades_2020_2024.csv # â† Step 1 output
â”‚   â”œâ”€â”€ labeled_regimes.csv            # â† Step 2 output
â”‚   â””â”€â”€ validation_report.txt          # â† Step 2 validation
â””â”€â”€ scripts/
    â”œâ”€â”€ extract_bear_trap_trades.py    # â† Step 1
    â”œâ”€â”€ label_regime_structural.py     # â† Step 2 (CORRECTED)
    â”œâ”€â”€ train_regime_model.py          # â† Step 4 (to create)
    â””â”€â”€ backtest_ml_bear_trap.py       # â† Step 5 (to create)
```

---

## ğŸ”¥ **What's Different from Original Plan?**

### **Original (Flawed):**
```
Step 2: Manual labeling based on outcomes
  â†“
"Trade won 2R â†’ ADD_ALLOWED"
  â†“
Conceptual look-ahead bias
```

### **Corrected:**
```
Step 2: Automated structural scoring
  â†“
"Strong trend + high volume + quick reclaim â†’ ADD_ALLOWED"
  â†“
No look-ahead bias
```

**Result:** 
- âœ… Faster (automated)
- âœ… More objective (scoring system)
- âœ… No bias (entry-time only)
- âœ… Reproducible (no subjective judgment)

---

## ğŸ“ˆ **Next Session (30-60 min)**

1. Run Step 1 (extract trades) - 10 min
2. Run Step 2 (label structurally) - 1 min
3. Review validation - 5 min
4. If validation passes â†’ proceed to ML training
5. If validation fails â†’ revise features and retry

---

**This corrected approach is bulletproof against look-ahead bias!** ğŸ¯
